# Dynamo

Dynamo 亚马逊的高可用Key-Value存储系统

---

通过阅读这篇论文，我们可以了解到Dynamo的如下特点：
    
* 数据库，最终一致性，可写至任意副本；这与PNUTS类似，但Dynamo的写操作总是成功的；和Bayou类似，但Dynamo有协调冲突的功能；Dynamo是一个令人惊诧的设计。
* 一个真正在生产中使用的系统，如亚马逊的购物车功能。
* 与PNUTS, Spanner相比，Dynamo有更高的可用性
* 与PNUTS, Spanner相比，Dynamo的一致性较低
* Dynamo的设计很有影响力，为其他分布式系统提供了灵感；如Cassandra（一套开源分布式NoSQL数据库系统）
* 2007年的论文，比PNUTS和Spanner都要早。

##设计目标

* SLA（Service Level Agreements）服务等级协议：如99.9%的请求延迟小于300ms
* 可以处理经常性的故障
* 数据中心被飓风摧毁后仍然保持可用
* 总是可写，即写操作总是可以成功


##设计架构

多个数据中心，多个Dynamo节点；每个数据项通过key哈希到到多个随机节点，多副本保存。
副本只存在少数几个站点，而不是所有站点，这是因为：若所有站点都保存，当只有两个数据中心时，一个站点发生故障会导致1/2的节点不可用，因此将任何数据都保存在所有站点时需要特别小心。而有10个数据中心时，一个站点发生故障后只会影响一小部分节点，因此只需要拷贝到少量站点。

##数据拆分 -- 一致性哈希

一致性哈希是一个由物理服务器组成的环状视图。
节点ID=random  节点的ID随机指定
key ID=hash(key)  key的ID通过hash函数获得
协调者：key所在位置的下一个服务器，client发送put/get请求到协调者上。协调者会将put/get请求发送给preference list中的服务器（“preference list”中记录了副本列表）。

**大部分远程连接的结果：**

* 许多put/get可能涉及WAN阻塞，导致高延迟;“quorums” will cut the tail end 
* 与PNUTS相比，面临网络故障会更脆弱


**为什么采用一致性哈希？**

* 优点：
    自然的均衡
    去中心---- 可以查找、加入/移除节点
* 缺点：
    并非真正的均衡，需要引入虚拟节点
    节点位置很难控制
    添加/删除节点时会改变数据划分，需要转移数据

##故障
    
首先要区分清楚是临时故障还是永久性故障？
如节点不可达时，若为临时故障，可以将put请求存储在其他节点直到故障节点恢复可用；若为永久性故障，需要为所有内容分配新的副本。Dynamo会将所有故障当做临时故障处理。


**总是可写**

总是可写=>没有master，必须能够写在本地。Dynamo采用了“sloppy quorums”，即马虎的多数算法，
总是可写+故障=版本冲突，Dynamo采用最终一致性，当没有故障时，sloppy quorums可以避免不一致。

##思路1：sloppy quorum

无故障时，尝试在单个master获取一致性，但是当协调者发生故障时也允许处理请求，这是PNUTS不能做到的
    当没有故障时，通过单一节点发送读/写,通常情况下，读会看到写。但并不一定，故障时，允许读写到任意节点。


**临时故障处理：quorum**

* 目标：不能在不可达节点上阻塞等待
* 目标：put方法应该总能成功
* 目标：get方法大部分情况下可以看到最近put的数据
* quorum：R + W > N （R读取数据的节点数量，W写入数据的节点数量，N节点总数）
    * 从不等待所有的N,即不会等待所有节点返回
    * R和W会发生重叠
    * 切断延迟并处理故障情况
* N是preference list中首个可达的节点数量，每个节点ping后续节点，对上下游节点粗略的估计
* sloppy quorum意味着R/W可以不重叠

**协调者处理put/get**

并发发送put/get请求到最开始的N个可达节点
put:等待W个节点回复请求
get:等待R个节点回复请求
如果故障不是太多，get会看到最近put的数据版本

若put()将数据放在环的远处时，故障修复后，新的数据数据是否属于N？服务器会记录数据真正应该属于的节点，之后一旦真正的节点可达，周期性同步key区间的默克尔树。

##思路#2 最终一致性

* 所有副本都接受写请求
* 允许副本中的数据有冲突
* 允许读到过期或冲突的数据
* 故障恢复时，需要解决多版本冲突；如果没有冲突，则使用最新的版本；如果有冲突，应用读出数据后需要合并后写入系统。


**最终一致性的缺点**

* 可能有多个最新的版本
* 读操作可能返回多个冲突的版本
* 应用必须能够合并以解决冲突
* 无原子操作（如：没有PNUTS的test-and-set-write）

**多个版本产生**

一个节点可能会由于网络原因错过了最新的写入，因此会保存旧数据，应该被最新的put()内容取代
get()时，向R个节点发送请求，可能看到最新版本和旧版本

**冲突版本产生**
```
    假设N=3 R=2 W=2
    购物车的起始内容为空字符串""
    preference list是n1, n1, n3, n4
    client 1 想要添加数据项X
        get()从n1, n2获取数据，返回""
        n1和n2发生故障
        put("X") 发送到n3, n4
    n1, n2恢复
    client 3 添加数据项Y
        get()从n1, n2获取数据，返回""
        put("Y") to n1, n2
    client 3想要显示购物车
        get() 从n1, n3 获取数据，返回了两个值"X" and "Y"
    这两个值不会互相覆盖对方，put()发生了冲突
```

**client如何处理读冲突？**
 
这依赖于应用如何处理;例如购物车的处理方式，其不会删除数据项X，（Bayou会直接删除，保存简单性）
一些应用可能使用最近的时间戳，若我更新了密码，应用应该使用最新的数据。
最后，将合并后的数据写回Dynamo


**编程API**

* 所有的对象都是不可变的
    get(k)可能返回多个版本，同时包含context
    put(k, v, context)为k创建一个新的版本，附加上context
* context用来合并和保持依赖，检测冲突，由对象版本向量组成

**版本向量(version vectors)**

```
    版本树示例1：
    [a:1]
           [a:1,b:2]
```
上面的版本向量了v2紧接着了v1，Dynamo的节点会丢弃[a:1]，而保存[a:1,b:2],
```
    版本树示例2：
    [a:1]
           [a:1,b:2]
    [a:2]
```
client必须合并两个分支    

**版本向量是否会变得很大？**

是的，但是很慢，由于key大多在相同的N个节点，版本向量的数量大于10个元素时，Dynamo会删除最旧的更新对应的版本实体。


**删除版本向量视图的影响**

Dynamo不会意识到一个版本包含另一个版本，其在不需要的时候会合并版本：
```
    put@b: [b:4]
    put@a: [a:3, b:4]
    forget b:4: [a:3]
```
现在，如果同步[b:4]，需要进行合并

丢弃最旧的版本是明智的,因为这个元素很可能出现在其他版本分支中，因此如果被遗漏会强制合并,丢弃最新的版本会抹去最近发生冲突的证据。（不是很清楚什么意思！！！）

**客户端合并冲突版本是否总是可能的？**

假设我们保持一个计数器x，x以0开始；增加两次，但是故障可能阻止客户端看到其他节点的写入，故障恢复后，client会看到两个版本，都是x=1
什么是正确的合并结果？客户端是否清楚合并？当然，客户端合并无从获知真实的技术结果，也不知道如何正确合并。

**无故障时，两个客户端并发写入，会发生什么？**

例如，两个client同时向相同的购物车增加不同的数据项，每个都是获取-修改-写入。
这两个客户端都将put()请求发送到了同一个协调者，协调者是否创建两个冲突的版本向量？我们想要想知道是否有一个会被丢弃。论文没有说明，但协调者可以通过put()的context检测到这个问题。

##永久性故障

永久性故障发生时，管理员手动修改服务列表；系统需要更新数据位置，这需要花费很长时间！

**问题**

* 需要花费一段时间才能让所有服务器注意到有添加或删除的服务器，这是否会造成麻烦？
* 删除的服务器可能将put()用于替换
* 删除的服务器可能遗漏一些put()后收到get()
* 增加的服务器可能由于协调者不知道导致遗漏一些put()
* 增加的服务器可能在完全初始化之前处理get()
* Dynamo可能做如下事情:
    类Quorum算法会导致get()看到最新的或过期的数据
    副本同步会修复遗漏的get()

##延迟

Dynamo的设计并不意味着低延迟，client可能被强制联系远程协调者；一些R/W节点距离可能较远，协调者必须等待。

**设计的哪些部分有助于限制99.9%的延迟？**

这个一个关于方差的问题，而不是平均数。等待多个服务器花费的时间是延迟最大的服务器所花费的时间，而不是平均时间；Dynamo只等待N台服务器中的W或R台，可通过配置W/R降低读写延迟。

##经验

**Amazon如何使用Dynamo？**

购物车（合并）
session信息（可能是最近的）
产品列表（主要是只读，副本用于高吞吐读操作）

**Dynamo的主要优势在于N/R/W的灵活性变化**

```
    N-R-W
    3-2-2 : 默认的、快速的读写, 合理的持久性
    3-3-1 : 快读写，慢读，并不是特别持久
    3-1-3 : 快读读、慢写、持久性一般
    3-3-3 : 减少读操作遗漏写入数据
    3-1-1 : 不是很有用？
```
  
**其他**

他们在数据划分/位置/负载均衡上瞎搞了一些时间，
    
    旧策略
        随机的节点ID意味着新节点需要拆分旧节点的区域，这需要扫描数据库的磁盘，代价十分昂贵
    
    新策略
        预先决定Q的集合，平均划分区间
        每个节点Q集合的协调者
        新节点负责整个区间中很少的部分
        每个区间存储在文件中，可以传输整个文件

**多个版本的好处：**

6.3 声明0.001%的读操作会看到相异的版本，我认为他们是指冲突版本，而不是多个版本。所有可能0.001%的写操作会用都“总是可写”这个功能。
    很难猜想：他们示意问题是并发写，对这个问题更好的方法是单一master；但是他们可能没有测量单master对可用性的影响

##性能/吞吐（图4,6.1）

图4中展现的平均10ms读，20ms写，20ms一定包含磁盘写，10ms可能包含等到N台的R/W
图4中可以看出：99.9%的时候大约时100到200ms的延迟。请求加载，对象数量，本地模式，这些意味着跨海岸消息传输时有时需要等待？

困惑：为什么图4中和表2中的平均延迟这么低？
    
我猜测他们很少等待网络延迟，但是第6节说了“多数据中心”，你期望大多数协调者和大多节点时远程的，但所有的数据中心可能距离西雅图都很近？


##总结

* 最终一致性
* 无论是否有故障发生，总是可写
* 允许有冲突的写入，有客户端合并解决冲突
* 对某些应用来说，Dynamo的模型很尴尬（过期读，合并），从论文中我们无从获知。
* 对获取高可用来说，可能是一个很好的方法，WAN无阻塞，但PNUTS的master策略暗示着并不是一个问题
* 最终一致性是否可以胜任存储系统还未取得一致的看法







