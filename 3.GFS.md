# GFS
---
    The Google File System
    Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung
    SOSP 2003

##为什么要学习这篇论文
* 这是map/reduce计算框架使用的文件系统
* 分析GFS并学习如何处理存储异常
* 如何简单高效的实现一致性
* 高性能的并行I/O
* 这篇论文系统地描述了分布式存储系统，从应用到网络的所有细节
* 论文包含了本课程的所有主题：性能、容错、一致性。

##一致性
一致性是指当数据存在副本时，多个应用读取时是否能够读到正确的数据。
展开来说，如果应用请求写入，写入后的一段时间后是否能够读取到正确的数据。如果是用于读取，是否也能读取到正确的数据？

**弱一致性**
read()可能返回过期数据，即不包含最近写入的数据

**强一致性**
read()总会返回最新write()写入的数据

**权衡**
强一致性对于writer来说很好，但也会带来性能问题

**一致性模型历史**
在架构、系统和数据库的发展过程中，都需要对一致性问题进行考虑，而一致性模型也各自独立发展，如：
并发处理器有各自私有的缓存，同时会共享内存
并发客户端访问分布式文件系统
分布式数据库的并发事务

不同的一致性模型有不同的权衡：

* 可串行化
* 顺序一致性
* 线性化
* 入口的一致性
* 释放一致性

**理想的一致性模型**

* 分布式文件系统中文件的行为与无副本文件系统中的文件一样，就像是同一台机器上，多个客户端访问同一个硬盘上的文件。
* 某个应用写入数据，稍后会读取到最新的数据。
* 如果两个应用同时向一个文件写入数据，文件系统通常是“未定义的”，即文件可能有很多混合的内容。
* 如果两个应用同时向同一个文件夹写入数据，应该一个先来，下一个继续。

**不一致的资源**
并发
计算机故障
网络分区

**GFS的例子**
primary指向的隔离的备份B
client 追加 1
primary 发送 1给自己和备份A
primary发送错误将错误报告给client
同时client2可能读取到备份B，观察到旧数据

**理想的一致性模型很难实现**

* 协议会变得很复杂（下周会讲到），很难正确的实现
* 协议需要clients和servers直接进行交互，对性能有影响

GFS的设计者为了更高的性能和设计的简洁性，放弃了理想的一致性模型。GFS主要在一致性、容错、性能和设计的简洁性上做了一个良好的权衡。

##GFS

**目标**
创建一个共享的分布式文件系统，运行在许多基于Linux的普通机器上，可以存储超大数据集。至于GFS存储的内容，作者没有明确说明，我们猜在2003年时候，最可能存储的内容是网站索引、数据库、所有的HTML文件和图像等等。这些文件的特征是：

* 总量TB级的数据集、
* 许多大文件，2003年的作者建议1M个文件，每个100MB。
* 文件只能append。

**主要挑战**

*  系统在日常运行过程中，有很多机器会发生故障。假设一台机器一年发生一次故障，1000台机器的集群，每天有3台机器会发生故障。
*  高性能，多个并行的Map/Reduce任务的readers和writers会读取和存储中间文件。
*  尽可能充分、有效的利用网络

**顶层设计**

* 文件夹、文件、命名、open/read/write，但不是POSIX规范
* 数据存储在多台chunk servers上，每个chunks（普通的Linux文件）设定为64MB，每个chunks在三台chunk servers上存有副本。保存三份副本的原因是为了提高数据的可用性，便于在发送故障时能够读取和恢复。除了可用性之外，三份副本可以还针对热点文件的读取进行负载均衡。GFS不使用RAID拷贝的原因是因为RAID很贵，并且需要对整台机器进行容错，而非只针对存储功能。
* GFS的master server保存元数据，如：文件的层级关系及文件的组织结构，以及文件所有chunk各自所在的chunk server位置。master将元数据保存在内存中。对于每个chunk需要64B的内存空间。除此之外，master还使用私有的数据库保存元数据，用于崩溃后迅速恢复状态。shadow masters比master的状态慢一些，发生故障时可以提升为master。


**基本操作**

* client读：client发送文件名和offset给master，master回复包含有该文件chunk的服务器列表，client缓存这些信息一段时间，最后向距离最近的服务器请求数据。
* client写：向master发送请求，如果超过64MB，master会选择一个新的chunk servers集，并选择其中一个作为primary，primary选择一个写入顺序并发送给其他两个备份，这些服务器都按照该顺序执行。

###容错机制

**master容错**

* client一直与master进行交互，且只有一个master，由master指定所有的操作顺序。
* 除了在内存中保存元数据外，master还持久化保存了namespace和文件与chunk的映射关系。并将操作记录到日志中。操作日志备份在多台服务器上，client所有修改元数据的操作都需要先记录到日志中才能返回。（日志在很多系统中都起到了重要作用）
* 日志会定期备份为"checkpoint",并移除"checkpoint"之前的所有日志文件，这保证了日志的大小不会过于庞大。checkpoint也需要进行备份。
* master需要恢复时，先选择最近的"checkpoint",再replay日志文件。chunk的位置信息由master向chunk server发出请求获取。
* 由于元数据很小，master的恢复速度很快，因此系统可能会在很短的时间内变得不可用。
* shadow masters的状态比master慢一些，会一直replay日志的副本。shadow在master崩溃时可以提供只读服务，但可能返回过期的数据。
* 如果master没有恢复，必须通过其他途径恢复master，但需要避免存在两个mster的情况

**Chunk 容错**

* 我们将接收Master颁发chunk“租约”的chunk server称之为primary。Primary决定了在chunk上所有副本server的操作顺序。
* Clients将数据发送给副本并链式传递给下一个副本。
* client向primary发送写请求，Primary会分配序列号，本地执行操作，并向其他副本发送请求，接收到所有副本响应后，向client回复写入结果。
* 如果其中一个副本未能响应，client会重试写操作。


**chunks server的一致性**

* 有些chunks server可能错过了修改信息，导致保存了过时数据。
* GFS通过chunk版本号检查chunk是否过期，master在颁发chunk“租约”之前，增加chunk的版本号，并将其发送给chunk副本所在的server。master和chunk server都持久化保存版本号。
* 版本号也会发送给client，方便其检查过期的数据。


**并发写/追加**

* 多个client可能并发的向同一个文件的相同区域写入文件，这可能造成写入的内容发生混合。但很少有应用会这么做，所有还算好啦。Unix上的并发写也可能会导致奇怪的结果。
* 许多client会并发的追加记录，如向日志文件添加记录。GFS支持原子性的、最少一次（失败重试）式的追加。primary chunk server为所有副本选择需要追加的offset。如果primary与其他副本沟通失败，会返回错误给client。client会重试；如果重试成功，有些副本可能会追加两次。如果追加会跨越块边界，文件可能会有“洞”。

**一致性模型**

* Master原子性的修改元数据，对文件夹操作是强一致性，但是当master离线时，只有shadow master提供只读操作，可能返回过时数据。

* chunk的操作采用弱一致性，一个失败的文件修改可能导致chunk不一致。如primary更新chunk，但是失败了，副本都过时了。client可能会读取到过时的chunk。
* GSF的作者认为弱一致性对应用来说不是一个很大的问题。很多文件只能追加操作来更新问题，应用可以使用追加记录中使用UID检测是否重复，应用程序可能只读取较少的数据。


**性能**

* 读取时由巨大的吞吐量:125MB/S,接近网络的容量。
* 写入到不同的文件，低于理论上的最大值，作者将其归结于他们的网络堆栈。将chunks从副本间传递也造成了延迟。
* 并发追加记录，受存储最后一个块的服务器限制。

##总结

* GFS使用的重要容错机制：日志，检查点、副本备份（一致性）
* 缺点：不适合存储小文件，master的容错机制差，不适合处理append之外的并发更新文件
