# P2P、DHT和Chord

内容主要包括：peer-to-peer (P2P)、BitTorrent、DHTs（Distributed Hash Table）、Chord算法。

##点对点

* 节点之间直接进行文件传输
* 用户节点不与中心服务器交互，而直接与其他节点进行交互以实现业务逻辑。
* 节点可以自由加入或退出。
* 例如：skype,和视频音乐播放器，文件分析

**优点**

* 传播文件的成本分布在每个用户的网络消耗
* 没有中心服务器意味着：容易部署、不容易过载、单个节点故障不会影响整个系统、很难受到攻击。

**缺点**

* 很难在数百万个节点上找到数据项
* 用户节点的可靠性比受管理的服务器低
* P2P网络如果是公开的，可能受到恶意参与者的攻击

**BitTorrent**

p2p网络的相关案例有：盗版文件共享程序；聊天程序如Skype和比特币等。典型的是BitTorrent：

* 一个流行的协同文件下载系统
* 用户点击下载链接（如最新的Linux内核），可以得到torrent文件。该文件内部包含需要下载文件的内容Hash和tracker的IP地址
* 用户的BT客户端连接tracker服务器，tracker回复保存下载文件的其他节点信息
* 用户节点与其他节点进行交互，获取该文件
* 下载完成后，用户节点通知tracker其已经获取了文件拷贝
* 用户节点下载完成后也为其他节点提供下载服务

##DHT

BitTorrent可以使用DHT代替tracker，这是今天这篇文章的主题

BT客户端一起合作实现了一个海量Key-Value存储系统（分布式哈希表）。key是文件内容哈希（infohash），value是提供下载的节点IP地址。
客户端节点调用get(infohash)找到其他提供下载服务的节点信息，put(infohash,self)将自己注册到可下载服务列表。客户端节点下载过程中也加入DHT开始提供服务。

**DHT较BitTorrent的优势**

单个存储大量数据的tracker服务器，与DHT相比，碎片化程度较小，因此客户端节点更容易找到其他节点。
tracker是一个典型的单点，过于暴露,容易受到攻击。

**DHT如何工作？**

可扩展的DHT查找：
key/value存储遍布于百万个节点，典型的DHT接口：

```
    put(key, value)
    get(key) -> value
```
宽松一致性：get(k)不保证看到put(k)，但保证数据的活性

DHT中查找信息很难，这是因为DHT中有数百万个参与的节点。请求可以进行广播发送，但是会有太多消息。每个节点需要知道其他每个节点。对key进行哈希很容易，但一直保持百万个节点的最新信息很难。因此，我们只能保存适量的状态，和适量的消息/查找。
 
**思路**

使用一个数据结构（如：树）保存其他节点信息，每个节点只保存少量其他节点的引用信息。查找时，首先检查该节点的数据结构（路由信息），若没有则跳转到下一个节点进行查找。DHT查找shi需要路由get()请求最终路由到之前put()时存储数据的节点。
例如："Chord"P2P查找系统；BitTorrent使用的Kademlia（由Chord启发的算法）

##Chord

Chord的ID空间拓扑结构是一个环：所有的ID都是160-bit的数字，首尾相连构成一个环。每个节点对应一个ID，随机选取生成。
Key存储在第一个ID等于或大于Key对应ID的节点。接近度被定义为“顺时针距离”。如果节点和key的ID是均衡的，我们就能得到合理的负载均衡，所以Key的ID应该被哈希（例如bittorrent的infohash）。

**基本路由**

查找节点时，节点需要将query转发到一个距离keyID更近的节点。如果我们一直将query转发到距离keyID最近的节点，最终我们会找到key所在的节点。
如果环上的每个节点都保存其在环上的下一个节点（称为successor），查找过程如下所示，在顺时针方向转发查找直到找到节点。
```
n.lookup(k):
    if n < k <= n.successor
        return n.successor
    else
        forward to n.successor
```
转发到successor的效率很慢，数据结构是一个链表，复杂度为O(n)。
我们是否可以使用二分查找？这样每步都将距离减半。

**哈希表路由**

节点以指数的形式保存多个节点状态：f[i]包含 n+2^i的successor，查找过程为：
```
n.lookup(k):
    if n < k <= n.successor:
        return successor
    else:
        n' = closest_preceding_node(k) -- in f[]
        forward to n'
```
对于一个6-bit的系统，哈希表可能是：
```
0: 14
1: 14
2: 14
3: 21
4: 32
5: 42
```
现在查找需要log(n)次转发,每条路由信息可以跳过一半的路程到达目标。
上述方法中，每个节点都保存一个二分查找树，这样可以跳过某些节点的路由表。这是一个更好的方法，每个节点作为根，所以没有节点作为热点，但是节点需要保存更多的后续节点信息。

**效率**

对于一个百万级的节点来说，需要20次跳转。如果每次跳转花费50ms，查找需要1秒左右；如果每次跳转有10%的机会失败，会出现几次超时。因此，实践中log(n)比O(n)更好。

**新节点如何获得正确的路由表？**

通常采用的方法：
假设系统以正确的路由表开始，我们使用路由表可以帮助新节点找到其所在的问题。因此添加新节点时需要维护正确的信息。
对于新节点m：我们先根据m对应的节点找到一个已存在的节点，即m.successor。m向其successor询问整个路由表。这样新节点即可获取正确的路由表。此时，新节点m可以正确转发请求。后台查找m+2^i个节点调整自己的路由表。

**路由到新节点m后的操作：stabilization**

若m获取路由表后什么都不做，m处理的key查找时会转发到m加入之前的那个节点。因此，m的predecessor需要设置其successor为m。每个节点都保存了自己当前的predecessor；当m加入时，通知其successor的predecessor发生了变化。节点周期性询问自己的successor的predecessor是否是自己。
因此，如果我们有x m y三个节点，x的successor是y (不正确)，y的predecessor是m，x向x的successor y询问其predecesso，y会返回m，x获得m后，设置x的successor为m，并通知m节点x是其predecessor。
我们维护正确的successors，以便于足够正确的查找！ 

**并发加入**

两个新节点有非常接近的ID，可能有相同的successor。
例如：初始时40加入，然后70加入；50和60并发加入；首先，40、50、60认为他们的successor是70，这意味着查找45时会找到70而不是50。在stabilization过程后，40和50会指向60，然后40会指向50。
为了维持log(n)的查找，节点加入时每个节点周期性查找其每个路由信息（n + 2^i）

**节点故障**

节点没有警告的情况下发生故障，这种情况比离去时的优雅更难，因为这面临两个问题：

* 其他节点的路由表引用了故障节点。
* 故障节点的predecessor没有了可通信的successor。

如果查找时路由到了故障节点，系统会检测到超时，将路由信息设置为空。因此，我们需要维护一个successors的列表，共维护r个successor节点信息。此时查找过程变为了查找大于等于key的活着的successor，或者转发到小于key的successor。

**处理不可达节点**

开放的p2p网络中，网络波动（Churn）会很高。用户会关闭他们的笔记本电脑，离开WIFI等等，这些都经常发生。Bittorrent/Kademlia的测试表明这种情况下查找并不是很快。

**地理/网络位置**

查找操作花费log(n)个消息，但是他们会被发到网络上的随机节点，距离可能会很远。我们是否能够路由到底层网络中距离我们较近的节点？

思路：收集哈希环中节点附近的n+2^i个节点，填充到路由表实体。可能还需要收集n+2^i的r个successors。使用多个路由信息中延迟最低的节点。



